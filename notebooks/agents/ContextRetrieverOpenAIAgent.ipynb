{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shaurya/projects/S2QA/.conda/lib/python3.10/site-packages/langchain/__init__.py:24: UserWarning: Importing BasePromptTemplate from langchain root module is no longer supported.\n",
      "  warnings.warn(\n",
      "/Users/shaurya/projects/S2QA/.conda/lib/python3.10/site-packages/langchain/__init__.py:24: UserWarning: Importing PromptTemplate from langchain root module is no longer supported.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import Sequence\n",
    "\n",
    "from llama_index import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_hub.semanticscholar.base import SemanticScholarReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     storage_context = StorageContext.from_defaults(persist_dir=\"./storage/biases\")\n",
    "#     march_index = load_index_from_storage(storage_context)\n",
    "\n",
    "#     storage_context = StorageContext.from_defaults(persist_dir=\"./storage/methods\")\n",
    "#     june_index = load_index_from_storage(storage_context)\n",
    "\n",
    "#     storage_context = StorageContext.from_defaults(persist_dir=\"./storage/datasets\")\n",
    "#     sept_index = load_index_from_storage(storage_context)\n",
    "\n",
    "#     index_loaded = False\n",
    "# except:\n",
    "#     index_loaded = False\n",
    "    \n",
    "index_loaded = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build indexes across the three data sources\n",
    "s2reader = SemanticScholarReader()\n",
    "\n",
    "if not index_loaded:\n",
    "    # load data\n",
    "    biases_docs = s2reader.load_data(\n",
    "        query=\"anti muslim biases in large language models\", limit=10\n",
    "    )\n",
    "    methods_docs = s2reader.load_data(\n",
    "        query=\"methods to reduce biases in large language models\", limit=10\n",
    "    )\n",
    "    datasets_docs = s2reader.load_data(\n",
    "        query=\"datasets to reduce biases in large language models\", limit=10\n",
    "    )\n",
    "    mitigation_strategies_docs = s2reader.load_data(\n",
    "        query=\"mitigation strategies for biases in large language models\", limit=10\n",
    "    )\n",
    "    evaluation_metrics_docs = s2reader.load_data(\n",
    "        query=\"evaluation metrics for bias detection in large language models\", limit=10\n",
    "    )\n",
    "    research_challenges_docs = s2reader.load_data(\n",
    "        query=\"research challenges in bias reduction for large language models\",\n",
    "        limit=10,\n",
    "    )\n",
    "    case_studies_docs = s2reader.load_data(\n",
    "        query=\"case studies of bias reduction in large language models\", limit=10\n",
    "    )\n",
    "\n",
    "    industry_practices_docs = s2reader.load_data(\n",
    "        query=\"industry practices in bias reduction for large language models\", limit=10\n",
    "    )\n",
    "    # build index\n",
    "    biases_index = VectorStoreIndex.from_documents(biases_docs)\n",
    "    methods_index = VectorStoreIndex.from_documents(methods_docs)\n",
    "    datasets_index = VectorStoreIndex.from_documents(datasets_docs)\n",
    "    mitigation_strategies_index = VectorStoreIndex.from_documents(\n",
    "        mitigation_strategies_docs\n",
    "    )\n",
    "    evaluation_metrics_index = VectorStoreIndex.from_documents(evaluation_metrics_docs)\n",
    "    research_challenges_index = VectorStoreIndex.from_documents(\n",
    "        research_challenges_docs\n",
    "    )\n",
    "    case_studies_index = VectorStoreIndex.from_documents(case_studies_docs)\n",
    "    industry_practices_index = VectorStoreIndex.from_documents(industry_practices_docs)\n",
    "\n",
    "    # persist index\n",
    "    biases_index.storage_context.persist(persist_dir=\"./storage/biases\")\n",
    "    methods_index.storage_context.persist(persist_dir=\"./storage/methods\")\n",
    "    datasets_index.storage_context.persist(persist_dir=\"./storage/datasets\")\n",
    "    mitigation_strategies_index.storage_context.persist(\n",
    "        persist_dir=\"./storage/mitigation_strategies\"\n",
    "    )\n",
    "    evaluation_metrics_index.storage_context.persist(\n",
    "        persist_dir=\"./storage/evaluation_metrics\"\n",
    "    )\n",
    "    research_challenges_index.storage_context.persist(\n",
    "        persist_dir=\"./storage/research_challenges\"\n",
    "    )\n",
    "    case_studies_index.storage_context.persist(persist_dir=\"./storage/case_studies\")\n",
    "    industry_practices_index.storage_context.persist(\n",
    "        persist_dir=\"./storage/industry_practices\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "biases_engine = biases_index.as_query_engine(similarity_top_k=3)\n",
    "methods_engine = methods_index.as_query_engine(similarity_top_k=3)\n",
    "datasets_engine = datasets_index.as_query_engine(similarity_top_k=3)\n",
    "mitigation_strategies_engine = mitigation_strategies_index.as_query_engine(\n",
    "    similarity_top_k=3\n",
    ")\n",
    "evaluation_metrics_engine = evaluation_metrics_index.as_query_engine(similarity_top_k=3)\n",
    "research_challenges_engine = research_challenges_index.as_query_engine(\n",
    "    similarity_top_k=3\n",
    ")\n",
    "case_studies_engine = case_studies_index.as_query_engine(similarity_top_k=3)\n",
    "industry_practices_engine = industry_practices_index.as_query_engine(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=biases_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"llm_biases\",\n",
    "            description=\"This has information about anti muslim biases in large language models. \"\n",
    "            \"Use a detailed plain text question as input to the tool.\",\n",
    "        ),\n",
    "    ),\n",
    "    QueryEngineTool(\n",
    "        query_engine=methods_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"llm_bias_methods\",\n",
    "            description=\"This has information about methods to reduce biases in large language models. \"\n",
    "            \"Use a detailed plain text question as input to the tool.\",\n",
    "        ),\n",
    "    ),\n",
    "    QueryEngineTool(\n",
    "        query_engine=datasets_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"llm_bias_datasets\",\n",
    "            description=\"This has information about datasets to reduce biases in large language models. \"\n",
    "            \"Use a detailed plain text question as input to the tool.\",\n",
    "        ),\n",
    "    ),\n",
    "    QueryEngineTool(\n",
    "        query_engine=mitigation_strategies_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"llm_bias_mitigation_strategies\",\n",
    "            description=\"This has information about mitigation strategies to reduce biases in large language models. \"\n",
    "            \"Use a detailed plain text question as input to the tool.\",\n",
    "        ),\n",
    "    ),\n",
    "    QueryEngineTool(\n",
    "        query_engine=evaluation_metrics_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"llm_bias_evaluation_metrics\",\n",
    "            description=\"This has information about evaluation metrics to measure biases in large language models. \"\n",
    "            \"Use a detailed plain text question as input to the tool.\",\n",
    "        ),\n",
    "    ),\n",
    "    QueryEngineTool(\n",
    "        query_engine=research_challenges_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"llm_bias_research_challenges\",\n",
    "            description=\"This tool contains information about the challenges faced in the field of bias research in large language models. \"\n",
    "            \"Use a detailed plain text question as input to the tool.\",\n",
    "        ),\n",
    "    ),\n",
    "    QueryEngineTool(\n",
    "        query_engine=case_studies_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"llm_bias_case_studies\",\n",
    "            description=\"This tool has information about case studies on biases in large language models. \"\n",
    "            \"Use a detailed plain text question as input to the tool.\",\n",
    "        ),\n",
    "    ),\n",
    "    QueryEngineTool(\n",
    "        query_engine=industry_practices_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"llm_bias_industry_practices\",\n",
    "            description=\"This tool provides information on industry practices for mitigating biases in large language models. \"\n",
    "            \"Use a detailed plain text question as input to the tool.\",\n",
    "        ),\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.schema import Document\n",
    "from llama_index.agent import ContextRetrieverOpenAIAgent\n",
    "\n",
    "\n",
    "# toy index - stores a list of abbreviations\n",
    "texts = [\"\"]\n",
    "docs = [Document(text=t) for t in texts]\n",
    "context_index = VectorStoreIndex.from_documents(docs)\n",
    "\n",
    "context_agent = ContextRetrieverOpenAIAgent.from_tools_and_retriever(\n",
    "    query_engine_tools, context_index.as_retriever(similarity_top_k=1), verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m\u001b[1;3mContext information is below.\n",
      "---------------------\n",
      "\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, either pick the corresponding tool or answer the function: Think of a new  research techniques on bias reduction in large language models.\n",
      "\n",
      "\u001b[0m=== Calling Function ===\n",
      "Calling function: llm_bias_research_challenges with args: {\n",
      "\"input\": \"research techniques on bias reduction in large language models\"\n",
      "}\n",
      "Got output: The paper \"Adversarial Filters of Dataset Biases\" presented at the International Conference on Machine Learning in 2020 investigates the use of adversarial filters to mitigate the overestimation of machine performance caused by dataset biases in large language models. The authors provide a theoretical understanding of the approach and demonstrate its effectiveness in reducing measurable dataset biases. They also show that models trained on filtered datasets exhibit better generalization to out-of-distribution tasks. The paper highlights the research challenges posed by filtered datasets and their potential impact on robust generalization.\n",
      "========================\n"
     ]
    }
   ],
   "source": [
    "response = context_agent.chat(\n",
    "    \"Think of a new  research techniques on bias reduction in large language models.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "One research technique on bias reduction in large language models is the use of adversarial filters to mitigate dataset biases. This technique involves identifying and filtering out biased examples from the training data to reduce the overestimation of machine performance caused by these biases.\n",
       "\n",
       "The research paper \"Adversarial Filters of Dataset Biases\" presented at the International Conference on Machine Learning in 2020 explores this approach. The authors provide a theoretical understanding of adversarial filters and demonstrate their effectiveness in reducing measurable dataset biases. They also show that models trained on filtered datasets exhibit better generalization to out-of-distribution tasks.\n",
       "\n",
       "This research technique addresses the challenge of dataset biases and their impact on model performance. By filtering out biased examples, the models can be trained to be more robust and generalize better to diverse tasks and domains.\n",
       "\n",
       "It is important to note that this is just one research technique among many others being explored in the field of bias reduction in large language models. Ongoing research aims to develop new and innovative techniques to further mitigate biases and improve the fairness and inclusivity of these models."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "# print markdown\n",
    "Markdown(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw GPT-4 Output\n",
    "\n",
    "In order to reduce anti-Muslim biases or any other type of biases in large language models, a variety of strategies can be applied:\n",
    "\n",
    "1. Diverse Training Data: The input data that is used to train algorithms should be diverse and include various types of linguistic input from different regions, communities, religions, and societies. If a model is trained on biased or unrepresentative data, it will produce biased results.\n",
    "\n",
    "2. Moderation and Regulation: Content moderation can be used to filter out any discriminatory, biased, or harmful content that may affect the learning of the model. This could include setting stringent rules for data collection, processing, and application.\n",
    "\n",
    "3. Use of Advanced Technologies: Machine learning and AI can be used to identify and eliminate any bias in the algorithm or data. Companies can leverage technologies like AI to detect prejudicial patterns and remove them.\n",
    "\n",
    "4. Bias Evaluation: Frequent evaluations and audits can help determine if any bias has seeped into the system. This will help maintain the neutrality and fairness of content.\n",
    "\n",
    "5. Raising Awareness: It is important to educate the AI community about the harm of biases and the need to actively avoid them during development.\n",
    "\n",
    "6. Strict Policies: Implementing strict anti-discrimination policies and practices within tech companies can help prevent biases from creeping into AI systems.\n",
    "\n",
    "7. Continuous learning and retraining: Language models should be trained on an ongoing basis with updated and diverse data to avoid any sort of biases.\n",
    "\n",
    "8. Inclusion of Ethics in AI: Including aspects of ethical considerations and principles in AI.\n",
    "\n",
    "The solution is a combination of developing robust technical approaches and fostering an inclusive culture in AI research and development.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
